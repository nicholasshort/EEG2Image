{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display, transform, read, split ...\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import splitfolders\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# image processing\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "\n",
    "# model / neural network\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-mention",
   "metadata": {},
   "source": [
    "### Step 2 - Data preprocessing\n",
    "\n",
    "To use your data (images), you have to pre-process them. \n",
    "\n",
    "#### 1. Visualize dataset images\n",
    "\n",
    "The first step is to display an image of each class to see what it looks like.\n",
    "\n",
    "Here, there is **5 classes** (for 5 flower types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95652192-e3e4-473f-ad7a-6712490b8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daisy\n",
    "img_daisy = image.load_img(\"/workspace/data/daisy/100080576_f52e8ee070_n.jpg\")\n",
    "img_daisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe9b75-42bc-4350-a678-804d2341e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dandelion\n",
    "img_dandelion = image.load_img(\"/workspace/data/dandelion/10043234166_e6dd915111_n.jpg\")\n",
    "img_dandelion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c83e35-6bb4-41b1-942e-864cb4b5cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roses\n",
    "img_roses = image.load_img(\"/workspace/data/roses/10090824183_d02c613f10_m.jpg\")\n",
    "img_roses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e23d6-2a9c-4348-a937-a8ca52573f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sunflowers\n",
    "img_sunflowers = image.load_img(\"/workspace/data/sunflowers/1008566138_6927679c8a.jpg\")\n",
    "img_sunflowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77058cf6-25f9-40ef-b930-f2927b441ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tulips\n",
    "img_tulips = image.load_img(\"/workspace/data/tulips/100930342_92e8746431_n.jpg\")\n",
    "img_tulips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72befee-de1a-4fdd-b161-1eea3eb242c3",
   "metadata": {},
   "source": [
    "#### 2. Split data to use a ResNet model\n",
    "\n",
    "By using a **ResNet** model, your dataset has to be split as follow:\n",
    "\n",
    "<img src=\"attachment:016f5918-d35a-4496-83a9-42843bbcd4fd.png\"  width=400>\n",
    "\n",
    "If this is not the case for your image classification dataset, follow the steps below.\n",
    "\n",
    "1. Have a separate dataset as follows: **one folder per class**.\n",
    "\n",
    "<img src=\"attachment:6cb999f7-3dfb-41df-a302-5115b27cb719.png\" width=800>\n",
    "\n",
    "2. Then play the following cell to split the dataset into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0644803-a69a-46e9-8bfe-47583407a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in a new folder named data-split\n",
    "splitfolders.ratio(\"/workspace/data\", output=\"/workspace/data-split\", seed=1337, ratio=(0.7, 0.2, 0.1), group_prefix=None, move=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17b7ddc-1b22-425d-b0dc-70a9bcfcfa42",
   "metadata": {},
   "source": [
    "Your data should, now, be split as follows:\n",
    "\n",
    "<img src=\"attachment:713d2de3-0894-411b-8e55-0820b5768e93.png\" width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28f282-87ab-42a0-8eb8-5ae077847947",
   "metadata": {},
   "source": [
    "#### 3. Create Keras data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c96f44c-8f93-4ac6-9175-2c4e2bc0a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classes name\n",
    "class_names = ['daisy','dandelion','roses','sunflowers','tulips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train_generator = datagen.flow_from_directory( \n",
    "    directory=\"/workspace/data-split/train/\", \n",
    "    classes = class_names,\n",
    "    target_size=(224, 224),  \n",
    "    batch_size=32, \n",
    "    class_mode=\"binary\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data\n",
    "valid_generator = datagen.flow_from_directory( \n",
    "    directory=\"/workspace/data-split/val/\", \n",
    "    classes = class_names,\n",
    "    target_size=(224, 224), \n",
    "    batch_size=32, \n",
    "    class_mode=\"binary\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42dd58e-7c93-4a07-a560-1b74aa341f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_generator = datagen.flow_from_directory( \n",
    "    directory=\"/workspace/data-split/test/\", \n",
    "    classes = class_names,\n",
    "    target_size=(224, 224), \n",
    "    batch_size=32, \n",
    "    class_mode=\"binary\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-morrison",
   "metadata": {},
   "source": [
    "### Step 3 - Build the model\n",
    "\n",
    "The first step is to build the model, using **ResNet50**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50 model\n",
    "resnet_50 = ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "for layer in resnet_50.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the entire model\n",
    "x = resnet_50.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(512, activation='relu')(x) \n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(256, activation='relu')(x) \n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(128, activation='relu')(x) \n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(64, activation='relu')(x) \n",
    "x = layers.Dropout(0.5)(x)\n",
    "predictions = layers.Dense(5, activation='softmax')(x)\n",
    "model = Model(inputs = resnet_50.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4962db-8a4e-434a-9d44-b8341d689c4f",
   "metadata": {},
   "source": [
    "### Step 4 - Train the model\n",
    "\n",
    "**Adam** optimizer is used to train the model over **10 epochs**. It is enough by using Transfer Learning.\n",
    "\n",
    "The loss is calculated with the **sparse_categorical_crossentropy** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training function\n",
    "def trainModel(model, epochs, optimizer):\n",
    "    batch_size = 32\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model.fit(train_generator, validation_data=valid_generator, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290216f-657d-4942-bb5d-794464a446d3",
   "metadata": {},
   "source": [
    "> Some warnings can appear, don't be afraid, you can execute the next steps of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch the training\n",
    "model_history = trainModel(model = model, epochs = 10, optimizer = \"Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c56399-04cd-4e7b-9b17-f270f9d42314",
   "metadata": {},
   "source": [
    "- Display **loss** curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_curve = model_history.history[\"loss\"]\n",
    "loss_val_curve = model_history.history[\"val_loss\"]\n",
    "plt.plot(loss_train_curve, label = \"Train\")\n",
    "plt.plot(loss_val_curve, label = \"Validation\")\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844fdd1f-2960-4064-97e8-767b09ab52dd",
   "metadata": {},
   "source": [
    "- Display **accuracy** curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd67b04-29fe-4d06-9fbe-a6106e72d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_curve = model_history.history[\"accuracy\"]\n",
    "acc_val_curve = model_history.history[\"val_accuracy\"]\n",
    "plt.plot(acc_train_curve, label = \"Train\")\n",
    "plt.plot(acc_val_curve, label = \"Validation\")\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d7004-dba5-489c-b1a0-5c6c949f900a",
   "metadata": {},
   "source": [
    "### Step 5 - Evaluate the model\n",
    "\n",
    "The model is evaluated on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69296b-5bc6-46ac-8b28-e3aa1e9a45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"The test loss is: \", test_loss)\n",
    "print(\"The best accuracy is: \", test_acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476ad32-4496-4e51-b42f-a130cb35e5e8",
   "metadata": {},
   "source": [
    "### Step 6 - Test the model on a new image\n",
    "\n",
    "To test your model and predict which classes new images belong to, you can import sounds into a /workspace/data_test folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626b98f-5085-4d49-9a08-e0aade03bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.preprocessing.image.load_img('/workspace/tulipe-test.jpeg', target_size=(224, 224))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = np.array([img_array]) \n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3ead9-d465-43f1-a989-2b725fecb858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for samples\n",
    "predictions = model.predict(img_array)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef2679-81de-4681-be27-d59653ab7ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate argmax for predictions\n",
    "class_id = np.argmax(predictions, axis = 1)\n",
    "print(class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8aa473-00b4-4dec-9f26-0c1241718ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform classes number into classes name\n",
    "class_names[class_id.item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f941624-584a-47e6-9f3a-bd4f68756459",
   "metadata": {},
   "source": [
    "### Step 7 - Save and export the model\n",
    "\n",
    ">To save your model, you should create an other Object Storage container (with write rights) and mount it in your workspace (`saved_model` in this example).\n",
    "\n",
    "You can now save your model in a dedicated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdafa90-8526-415e-9344-da08d5e5ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/workspace/saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d56c37-f260-4880-9acd-eb6f1c1e3859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model directory\n",
    "%ls /workspace/saved_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40302468-c5af-4b63-a6d1-eacdc63d5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains an assets folder, saved_model.pb, and variables folder\n",
    "%ls /workspace/saved_model/my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b5de7-6a63-4571-aa15-acbbd0fce6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('/workspace/saved_model/my_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ebcf1-2138-4c46-80f4-db94079fa229",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Transfer Learning** saves time by achieving better performance in fewer epochs. \n",
    "\n",
    "Train and test this model on your own dataset !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
